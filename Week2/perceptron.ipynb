{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOv6WCb3w5f9I0WqHSWy44C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W8K5McfQzjx1"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["def step_func(x):\n","    # x is a numpy array of size 1 here\n","    return np.where(x >= 0, 1, 0)"],"metadata":{"id":"RjRkr7z9oMn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Perceptron:\n","    def __init__(self, input_size):\n","        self.weights = np.random.rand(input_size)\n","        self.bias = np.random.rand(1)\n","\n","    def forward_propagation(self, X):\n","        z = self.weights@X.T + self.bias\n","        return step_func(z)\n","\n","    def back_propagation(self, training_inputs, y, iterations=10, learning_rate=0.1):\n","        for _ in range(iterations):\n","            variation = self.forward_propagation(training_inputs) - y # shape = (len(y),) or (len(training_data),)\n","            dbias = np.sum(2*variation)\n","            dweights =  variation@training_inputs # shape = (input_size,)\n","            self.bias -= learning_rate*dbias\n","            self.weights -= learning_rate*dweights\n","\n","    def show(self):\n","        print(self.weights, self.bias)\n"],"metadata":{"id":"wsoI7JD2oElu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Works well for the AND operator because the output can be determined by finding in which half plane do the inputs lie"],"metadata":{"id":"Y7PvMOFpk0xW"}},{"cell_type":"code","source":["# Set operator table for AND\n","training_inputs = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n","y = np.array([0, 0, 0, 1])\n","operator = 'AND'"],"metadata":{"id":"rHIOp9M4UQ3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Doesn't work for the XOR operator because the output can't be determined by finding in which half plane do the inputs lie"],"metadata":{"id":"4-Ygny_vlo6U"}},{"cell_type":"code","source":["# Set operator table for XOR\n","training_inputs = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n","y = np.array([0, 1, 1, 0])\n","operator = 'XOR'"],"metadata":{"id":"YC7DgHtEV4e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the perceptron\n","print(\"Operator being trained for is:\", operator)\n","perceptron = Perceptron(2)\n","print('Weights and bias before training: ', end='')\n","perceptron.show()\n","print('Outputs before training:', perceptron.forward_propagation(training_inputs))\n","perceptron.back_propagation(training_inputs, y)\n","print('Weights and bias after training: ', end='')\n","perceptron.show()\n","print('Outputs after training:', perceptron.forward_propagation(training_inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfQ-AG8yUSt_","executionInfo":{"status":"ok","timestamp":1735659099154,"user_tz":-330,"elapsed":407,"user":{"displayName":"Vedant Saini","userId":"15689663066943030369"}},"outputId":"eae1a3ad-270e-480b-ff42-5093df2f2849"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Weights and bias before training: [0.73300182 0.18307079] [0.71237566]\n","Outputs before training: [1 1 1 1]\n","Weights and bias after training: [0.43300182 0.28307079] [-0.68762434]\n","Outputs after training: [0 0 0 1]\n"]}]}]}